{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enjoyPG/2023Gifted/blob/main/students/jwoo428/pytorch/10%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dc1yPE3hluG",
        "outputId": "41e87a5d-a1b6-45a4-fa13-f7381b017cbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 살펴보기"
      ],
      "metadata": {
        "id": "6iWIw3d9hofA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUYHv3QyXBq5",
        "outputId": "fb670e7d-8116-4496-9345-aba4b948647d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
            "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/CH10/ArticlesApril2017.csv\")\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습용 데이터셋 정의"
      ],
      "metadata": {
        "id": "E6Vd-BH4h2AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class TextGeneration(Dataset):\n",
        "    def clean_text(self, txt):\n",
        "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
        "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "        return txt\n",
        "    def __init__(self):\n",
        "        all_headlines = []\n",
        "\n",
        "        # ❶ 모든 헤드라인의 텍스트를 불러옴\n",
        "        for filename in glob.glob(\"/content/drive/MyDrive/Colab Notebooks/data/CH10/*.csv\"):\n",
        "            if 'Articles' in filename:\n",
        "                article_df = pd.read_csv(filename)\n",
        "\n",
        "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
        "                all_headlines.extend(list(article_df.headline.values))\n",
        "                break\n",
        "\n",
        "        # ❷ headline 중 unknown 값은 제거\n",
        "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "        \n",
        "        # ❸ 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
        "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
        "        self.BOW = {}\n",
        "\n",
        "        # ➍ 모든 문장의 단어를 추출해 고유번호 지정\n",
        "        for line in self.corpus:\n",
        "            for word in line.split():\n",
        "                if word not in self.BOW.keys():\n",
        "                    self.BOW[word] = len(self.BOW.keys())\n",
        "\n",
        "        # 모델의 입력으로 사용할 데이터\n",
        "        self.data = self.generate_sequence(self.corpus)\n",
        "    def generate_sequence(self, txt):\n",
        "        seq = []\n",
        "\n",
        "        for line in txt:\n",
        "            line = line.split()\n",
        "            line_bow = [self.BOW[word] for word in line]\n",
        "\n",
        "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
        "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) \n",
        "            for i in range(len(line_bow)-2)]\n",
        "            \n",
        "            seq.extend(data)\n",
        "\n",
        "        return seq\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        data = np.array(self.data[i][0])  # ❶ 입력 데이터\n",
        "        label = np.array(self.data[i][1]).astype(np.float32)  # ❷ 출력 데이터\n",
        "\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "1QtO1ZfsXmJh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM모델 정의"
      ],
      "metadata": {
        "id": "Mx0YMUARiK9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "   def __init__(self, num_embeddings):\n",
        "       super(LSTM, self).__init__()\n",
        "\n",
        "       # ❶ 밀집표현을 위한 임베딩층\n",
        "       self.embed = nn.Embedding(\n",
        "           num_embeddings=num_embeddings, embedding_dim=16)\n",
        "       \n",
        "       # LSTM을 5개층을 쌓음\n",
        "       self.lstm = nn.LSTM(\n",
        "           input_size=16, \n",
        "           hidden_size=64, \n",
        "           num_layers=5, \n",
        "           batch_first=True)\n",
        "       \n",
        "       # 분류를 위한 MLP층\n",
        "       self.fc1 = nn.Linear(128, num_embeddings)\n",
        "       self.fc2 = nn.Linear(num_embeddings,num_embeddings)\n",
        "\n",
        "       # 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = self.embed(x)\n",
        "\n",
        "       # ❷ LSTM 모델의 예측값\n",
        "       x, _ = self.lstm(x)\n",
        "       x = torch.reshape(x, (x.shape[0], -1))\n",
        "       x = self.fc1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.fc2(x)\n",
        "\n",
        "       return x"
      ],
      "metadata": {
        "id": "iQ_NQkA7llFJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습하기"
      ],
      "metadata": {
        "id": "zHbQlFoZiRN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 학습을 진행할 프로세서 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "dataset = TextGeneration()  # 데이터셋 정의\n",
        "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)  # 모델 정의\n",
        "loader = DataLoader(dataset, batch_size=64)\n",
        "optim = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "   iterator = tqdm.tqdm(loader)\n",
        "   for data, label in iterator:\n",
        "       # 기울기 초기화\n",
        "       optim.zero_grad()\n",
        "\n",
        "       # 모델의 예측값\n",
        "       pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
        "\n",
        "       # 정답 레이블은 long 텐서로 반환해야 함\n",
        "       loss = nn.CrossEntropyLoss()(\n",
        "           pred, torch.tensor(label, dtype=torch.long).to(device))\n",
        "       \n",
        "       # 오차 역전파\n",
        "       loss.backward()\n",
        "       optim.step()\n",
        "\n",
        "       iterator.set_description(f\"epoch{epoch} loss:{loss.item()}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"lstm.pth\")"
      ],
      "metadata": {
        "id": "E9x_OCUjnGe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfbffda-e4a6-48d7-d3a0-ce3b86801274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/104 [00:00<?, ?it/s]<ipython-input-5-a98fe878d097>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
            "<ipython-input-5-a98fe878d097>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred, torch.tensor(label, dtype=torch.long).to(device))\n",
            "epoch0 loss:7.395105361938477: 100%|██████████| 104/104 [00:27<00:00,  3.76it/s]\n",
            "epoch1 loss:6.937687397003174: 100%|██████████| 104/104 [00:26<00:00,  3.96it/s]\n",
            "epoch2 loss:6.403810501098633: 100%|██████████| 104/104 [00:24<00:00,  4.23it/s]\n",
            "epoch3 loss:5.827237129211426: 100%|██████████| 104/104 [00:22<00:00,  4.53it/s]\n",
            "epoch4 loss:5.525407791137695: 100%|██████████| 104/104 [00:22<00:00,  4.69it/s]\n",
            "epoch5 loss:5.47242546081543: 100%|██████████| 104/104 [00:20<00:00,  5.01it/s]\n",
            "epoch6 loss:5.534168720245361: 100%|██████████| 104/104 [00:21<00:00,  4.95it/s]\n",
            "epoch7 loss:5.23973274230957: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s]\n",
            "epoch8 loss:5.5326032638549805: 100%|██████████| 104/104 [00:20<00:00,  5.16it/s]\n",
            "epoch9 loss:6.165681838989258: 100%|██████████| 104/104 [00:20<00:00,  5.03it/s]\n",
            "epoch10 loss:6.941052436828613: 100%|██████████| 104/104 [00:20<00:00,  5.16it/s]\n",
            "epoch11 loss:6.710087776184082: 100%|██████████| 104/104 [00:20<00:00,  5.09it/s]\n",
            "epoch12 loss:6.619915008544922:  92%|█████████▏| 96/104 [00:18<00:01,  4.35it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
        "   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "   print(f\"input word: {string}\")\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for p in range(strlen):\n",
        "           # 입력 문장을 텐서로 변경\n",
        "           words = torch.tensor(\n",
        "               [BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
        "\n",
        "           # ❶\n",
        "           input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
        "           output = model(input_tensor)  # 모델을 이용해 예측\n",
        "           output_word = (torch.argmax(output).cpu().numpy())\n",
        "           string += list(BOW.keys())[output_word]  # 문장에 예측된 단어를 추가\n",
        "           string += \" \"\n",
        "\n",
        "   print(f\"predicted sentence: {string}\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n",
        "pred = generate(model, dataset.BOW)"
      ],
      "metadata": {
        "id": "n2aJFOvbn3Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "28rPwg8SxImr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}