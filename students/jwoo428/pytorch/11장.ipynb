{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enjoyPG/2023Gifted/blob/main/students/jwoo428/pytorch/11%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TzpHyKhSShGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40add1b5-8670-4005-a6f5-a7574f8cc29b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 확인해보기"
      ],
      "metadata": {
        "id": "7hN7LNjvklsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "l = []\n",
        "\n",
        "\n",
        "# 한글 텍스트 파일을 읽기 위해 utf-8 인코딩으로 읽어옴\n",
        "with open(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/data/CH11.txt\", \n",
        "    'r', encoding=\"utf-8\") as f:\n",
        "   lines = f.read().split(\"\\n\")\n",
        "   for line in lines:\n",
        "       # 특수 문자를 지우고 모든 글자를 소문자로 변경\n",
        "       txt = \"\".join(v for v in line if v not in string.punctuation).lower()\n",
        "       l.append(txt)\n",
        "\n",
        "print(l[:5])"
      ],
      "metadata": {
        "id": "_jmEcVHzkm3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1319bd-5cfd-4176-c851-d6689c6f6329"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['go\\t가', 'hi\\t안녕', 'run\\t뛰어', 'run\\t뛰어', 'who\\t누구']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BOW를 만드는 함수 정의"
      ],
      "metadata": {
        "id": "y2UcLB0plJ_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "def get_BOW(corpus):  # 문장들로부터 BOW를 만드는 함수\n",
        "   BOW = {\"<SOS>\":0, \"<EOS>\":1}  # <SOS> 토큰과 <EOS> 토큰을 추가\n",
        "\n",
        "   #문장 내 단어들을 이용해 BOW를 생성\n",
        "   for line in corpus:\n",
        "       for word in line.split():\n",
        "           if word not in BOW.keys():\n",
        "               BOW[word] = len(BOW.keys())\n",
        "\n",
        "   return BOW"
      ],
      "metadata": {
        "id": "dJhKq_3rlKoS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습에 사용할 데이터셋 정의"
      ],
      "metadata": {
        "id": "6POM6L3ulRfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Eng2Kor(Dataset):  # 학습에 이용할 데이터셋\n",
        "   def __init__(\n",
        "       self, \n",
        "       pth2txt=\\\n",
        "       \"/content/drive/MyDrive/Colab Notebooks/data/CH11.txt\"):\n",
        "       self.eng_corpus = []  # 영어 문장이 들어가는 변수\n",
        "       self.kor_corpus = []  # 한글 문장이 들어가는 변수\n",
        "\n",
        "       # 텍스트 파일을 읽어서 영어 문장과 한글 문장을 저장\n",
        "       with open(pth2txt, 'r', encoding=\"utf-8\") as f:\n",
        "           lines = f.read().split(\"\\n\")\n",
        "           for line in lines:\n",
        "               # 특수 문자와 대문자 제거\n",
        "               txt = \"\".join(\n",
        "                   v for v in line if v not in string.punctuation\n",
        "                   ).lower()\n",
        "               engtxt = txt.split(\"\\t\")[0]\n",
        "               kortxt = txt.split(\"\\t\")[1]\n",
        "\n",
        "               # 길이가 10 이하인 문장만을 사용\n",
        "               if len(engtxt.split()) <= 10 and len(kortxt.split()) <= 10:\n",
        "                   self.eng_corpus.append(engtxt)\n",
        "                   self.kor_corpus.append(kortxt)\n",
        "\n",
        "       self.engBOW = get_BOW(self.eng_corpus)  # 영어 BOW\n",
        "       self.korBOW = get_BOW(self.kor_corpus)  # 한글 BOW\n",
        "   # 문장을 단어별로 분리하고 마지막에 <EOS>를 추가\n",
        "   def gen_seq(self, line):\n",
        "       seq = line.split()\n",
        "       seq.append(\"<EOS>\")\n",
        "\n",
        "       return seq\n",
        "   def __len__(self): # ❶ \n",
        "       return len(self.eng_corpus)\n",
        "\n",
        "   def __getitem__(self, i): # ❷\n",
        "       # 문자열로 되어 있는 문장을 숫자 표현으로 변경\n",
        "       data = np.array([\n",
        "            self.engBOW[txt] for txt in self.gen_seq(self.eng_corpus[i])])\n",
        "       \n",
        "       label = np.array([\n",
        "            self.korBOW[txt] for txt in self.gen_seq(self.kor_corpus[i])])\n",
        "\n",
        "       return data, label"
      ],
      "metadata": {
        "id": "PSPo_poKlSLB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습에 사용할 데이터 로더 정의"
      ],
      "metadata": {
        "id": "85apUvjIlble"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loader(dataset):  # 데이터셋의 문장을 한문장씩 불러오기 위한 함수\n",
        "   for i in range(len(dataset)):\n",
        "       data, label = dataset[i]\n",
        "\n",
        "       # ❶ 데이터와 정답을 반환\n",
        "       yield torch.tensor(data), torch.tensor(label)"
      ],
      "metadata": {
        "id": "4cmyhHzvlcwe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인코더 정의"
      ],
      "metadata": {
        "id": "bu3VN0CzlpPi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7uyIraNiv0sl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "   def __init__(self, input_size, hidden_size):\n",
        "       super(Encoder, self).__init__()\n",
        "\n",
        "       self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "       self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "   def forward(self, x, h):\n",
        "       # 배치차원과 시계열 차원 추가\n",
        "       x = self.embedding(x).view(1, 1, -1)\n",
        "       output, hidden = self.gru(x, h)\n",
        "       return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 디코더 정의"
      ],
      "metadata": {
        "id": "qa0sxwSNl6D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "   def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=11):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "       # 임베딩층 정의\n",
        "       self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "       # 어텐션 가중치를 계산하기 위한 MLP층\n",
        "       self.attention = nn.Linear(hidden_size * 2, max_length)\n",
        "\n",
        "       #특징 추출을 위한 MLP층\n",
        "       self.context = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "       # 과적합을 피하기 위한 드롭아웃 층\n",
        "       self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "       # GRU층\n",
        "       self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "       # 단어 분류를 위한 MLP층\n",
        "       self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "       # 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "   def forward(self, x, h, encoder_outputs):\n",
        "       # 입력을 밀집 표현으로\n",
        "       x = self.embedding(x).view(1, 1, -1)\n",
        "       x = self.dropout(x)\n",
        "\n",
        "       # 어텐션 가중치 계산\n",
        "       attn_weights = self.softmax(\n",
        "           self.attention(torch.cat((x[0], h[0]), -1)))\n",
        "       \n",
        "       # 어텐션 가중치와 인코더의 출력을 내적\n",
        "       attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                encoder_outputs.unsqueeze(0))\n",
        "       \n",
        "       # 인코더 각 시점의 중요도와 민집표현을 합쳐\n",
        "       # MLP층으로 특징 추출\n",
        "       output = torch.cat((x[0], attn_applied[0]), 1)\n",
        "       output = self.context(output).unsqueeze(0)\n",
        "       output = self.relu(output)\n",
        "\n",
        "       # GRU층으로 입력\n",
        "       output, hidden = self.gru(output, h)\n",
        "\n",
        "       # 예측된 단어 출력\n",
        "       output = self.out(output[0])\n",
        "\n",
        "       return output"
      ],
      "metadata": {
        "id": "YtgwsDwpl3Au"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습에 필요한 요소 정의"
      ],
      "metadata": {
        "id": "kP9exMqrmAXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tqdm\n",
        "\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "\n",
        "# 학습에 사용할 프로세서 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# 학습에 사용할 데이터셋 정의\n",
        "dataset = Eng2Kor()\n",
        "\n",
        "\n",
        "# 인코더 디코더 정의\n",
        "encoder = Encoder(input_size=len(dataset.engBOW), hidden_size=64).to(device)\n",
        "decoder = Decoder(64, len(dataset.korBOW), dropout_p=0.1).to(device)\n",
        "# 인코더 디코더 학습을 위한 최적화 정의\n",
        "encoder_optimizer = Adam(encoder.parameters(), lr=0.0001)\n",
        "decoder_optimizer = Adam(decoder.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "7bkcuNpCl8T_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 루프 정의"
      ],
      "metadata": {
        "id": "riaFqRGnmQ6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(200):\n",
        "   iterator = tqdm.tqdm(loader(dataset), total=len(dataset))\n",
        "   total_loss = 0\n",
        "\n",
        "   for data, label in iterator:\n",
        "       data = torch.tensor(data, dtype=torch.long).to(device)\n",
        "       label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "       # 인코더의 초기 은닉 상태\n",
        "       encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "       # 인코더의 모든 시점의 출력을 저장하는 변수\n",
        "       encoder_outputs = torch.zeros(11, 64).to(device)\n",
        "\n",
        "       encoder_optimizer.zero_grad()\n",
        "       decoder_optimizer.zero_grad()\n",
        "\n",
        "       loss = 0\n",
        "       for ei in range(len(data)):\n",
        "           # 한 단어씩 인코더에 넣어줌\n",
        "           encoder_output, encoder_hidden = encoder(\n",
        "               data[ei], encoder_hidden)  \n",
        "           # 인코더의 은닉 상태를 저장\n",
        "           encoder_outputs[ei] = encoder_output[0, 0]  \n",
        "\n",
        "       decoder_input = torch.tensor([[0]]).to(device)\n",
        "       \n",
        "       # 인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로 저장\n",
        "       decoder_hidden = encoder_hidden\n",
        "       use_teacher_forcing = True if random.random() < 0.5 else False  # \n",
        "\n",
        "       if use_teacher_forcing:\n",
        "           for di in range(len(label)):\n",
        "               decoder_output = decoder(\n",
        "                   decoder_input, decoder_hidden, encoder_outputs)\n",
        "               \n",
        "               # 직접적으로 정답을 다음 시점의 입력으로 넣어줌\n",
        "               target = torch.tensor(label[di], dtype=torch.long).to(device)  \n",
        "               target = torch.unsqueeze(target, dim=0).to(device)\n",
        "               loss += nn.CrossEntropyLoss()(decoder_output, target)\n",
        "               decoder_input = target\n",
        "       else:\n",
        "           for di in range(len(label)):\n",
        "               decoder_output = decoder(\n",
        "                   decoder_input, decoder_hidden, encoder_outputs)\n",
        "               \n",
        "               # 가장 높은 확률을 갖는 단어의 인덱스가 topi\n",
        "               topv, topi = decoder_output.topk(1)  \n",
        "               decoder_input = topi.squeeze().detach()\n",
        "\n",
        "               # 디코더의 예측값을 다음 시점의 입력으로 넣어줌\n",
        "               target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
        "               target = torch.unsqueeze(target, dim=0).to(device)\n",
        "               loss += nn.CrossEntropyLoss()(decoder_output, target)\n",
        "\n",
        "               if decoder_input.item() == 1:  # <EOS> 토큰을 만나면 중지\n",
        "                   break\n",
        "       # 전체 손실 계산\n",
        "       total_loss += loss.item()/len(dataset)\n",
        "       iterator.set_description(f\"epoch:{epoch+1} loss:{total_loss}\")\n",
        "       loss.backward()\n",
        "\n",
        "       encoder_optimizer.step()\n",
        "       decoder_optimizer.step()\n",
        "\n",
        "torch.save(encoder.state_dict(), \"attn_enc.pth\")\n",
        "torch.save(decoder.state_dict(), \"attn_dec.pth\")"
      ],
      "metadata": {
        "id": "Tl0YqShCmCO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0dc088-15ef-42e4-e3e8-91d61d1c39ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3592 [00:00<?, ?it/s]<ipython-input-9-70ecbeb6059f>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data, dtype=torch.long).to(device)\n",
            "<ipython-input-9-70ecbeb6059f>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label, dtype=torch.long).to(device)\n",
            "<ipython-input-9-70ecbeb6059f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
            "epoch:1 loss:0.0047742481486569:   0%|          | 1/3592 [00:00<13:51,  4.32it/s]<ipython-input-9-70ecbeb6059f>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
            "epoch:1 loss:23.19099489026981: 100%|██████████| 3592/3592 [01:48<00:00, 33.01it/s]\n",
            "epoch:2 loss:22.343167052701737: 100%|██████████| 3592/3592 [02:16<00:00, 26.22it/s]\n",
            "epoch:3 loss:6.984907484333327:  42%|████▏     | 1494/3592 [00:47<01:02, 33.47it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 성능 평가에 필요한 요소 정의"
      ],
      "metadata": {
        "id": "xWsUZX4f2fV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 가중치 불러오기\n",
        "encoder.load_state_dict(torch.load(\"attn_enc.pth\", map_location=device))\n",
        "# 디코더 가중치 불러오기\n",
        "decoder.load_state_dict(torch.load(\"attn_dec.pth\", map_location=device))\n",
        "\n",
        "\n",
        "# 불러올 영어 문장을 랜덤하게 지정\n",
        "idx = random.randint(0, len(dataset))\n",
        "# 테스트에 사용할 문장\n",
        "input_sentence = dataset.eng_corpus[idx]\n",
        "# 신경망이 번역한 문장\n",
        "pred_sentence = \"\"\n",
        "\n",
        "data, label = dataset[idx]\n",
        "data = torch.tensor(data, dtype=torch.long).to(device)\n",
        "label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "# 인코더의 초기 은닉 상태 정의\n",
        "encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "# 인코더 출력을 담기위한 변수\n",
        "encoder_outputs = torch.zeros(11, 64).to(device)"
      ],
      "metadata": {
        "id": "96gq4Lphm1Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인코더 동작"
      ],
      "metadata": {
        "id": "o9MzH8bN2z5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ei in range(len(data)):\n",
        "   # 한 단어씩 인코더에 넣어줌\n",
        "   encoder_output, encoder_hidden = encoder(\n",
        "       data[ei], encoder_hidden)\n",
        "     \n",
        "   # 인코더의 출력을 저장\n",
        "   encoder_outputs[ei] = encoder_output[0, 0]  \n",
        "\n",
        "\n",
        "# 디코더의 초기 입력\n",
        "# 0은 <SOS>토큰\n",
        "decoder_input = torch.tensor([[0]]).to(device)\n",
        "\n",
        "# 인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로\n",
        "decoder_hidden = encoder_hidden "
      ],
      "metadata": {
        "id": "la5BGiB520bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 디코더 동작"
      ],
      "metadata": {
        "id": "do9gDbpd22LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for di in range(11):\n",
        "    #가장 높은 확률을 갖는 단어의 요소를 구함\n",
        "   decoder_output = decoder(\n",
        "                       decoder_input, decoder_hidden, encoder_outputs)\n",
        "   topv, topi = decoder_output.topk(1)\n",
        "   decoder_input = topi.squeeze().detach()\n",
        "\n",
        "   #<EOS> 토큰을 만나면 중지\n",
        "   if decoder_input.item() == 1:  \n",
        "       break\n",
        "\n",
        "   #가장 높은 단어를 문자열에 추가\n",
        "   pred_sentence += list(dataset.korBOW.keys())[decoder_input] + \" \"  \n",
        "\n",
        "print(input_sentence)  # 영어 문장\n",
        "print(pred_sentence)  # 한글 문장"
      ],
      "metadata": {
        "id": "B2moagtO25b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_oYu7Cd_VH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}